# Text-to-Image Grader Redesign Architecture

## üìê Design Philosophy

The redesigned grader follows a **hierarchical metric structure** with clear separation between:
1. **North Star Metric** (Primary quality indicator)
2. **Supporting Metrics** (Multi-dimensional quality assessment)

---

## ‚≠ê North Star Metric: Soft-TIFA GM

### What is Soft-TIFA?
**Soft-TIFA (Soft Text-Image Faithfulness through Atomic evaluation)** measures how well an image satisfies atomic visual criteria extracted from the prompt.

### Why Geometric Mean (GM)?
- **Compositional AND Logic:** All criteria must be satisfied (not just averaged)
- **Penalizes Missing Elements:** A single 0 score results in 0 overall score
- **Balanced Assessment:** Better reflects human judgment on completeness

### Methodology:
```
1. Extract 5-8 atomic facts from prompt
   Examples: "a red car", "on a highway", "sunny weather"

2. Score each atom probabilistically (0.0 to 1.0)
   VLM evaluates: Is this criterion met in the image?

3. Calculate Geometric Mean:
   Soft-TIFA GM = (score‚ÇÅ √ó score‚ÇÇ √ó ... √ó score‚Çô)^(1/n) √ó 100
```

### Example:
```
Prompt: "A red car on a highway with mountains in background"

Atoms:
- "A car is present" ‚Üí 1.0 ‚úì
- "The car is red" ‚Üí 0.9 ‚úì (slightly orange-red)
- "Car is on a highway" ‚Üí 0.8 ‚úì (looks like highway)
- "Mountains in background" ‚Üí 0.0 ‚úó (no mountains visible)

Soft-TIFA GM = (1.0 √ó 0.9 √ó 0.8 √ó 0.0)^(1/4) √ó 100 = 0/100
```

**Result:** Low score because mountains are missing (compositional AND logic).

---

## üìä Supporting Metrics

### A. Image-Only Quality Metrics
**Purpose:** Assess technical quality independent of prompt

| Metric | What It Measures |
|--------|------------------|
| **BRISQUE** | Blind spatial quality (blur, noise, compression artifacts) |
| **NIQE** | Natural image statistics (deviation from natural distributions) |
| **CLIP-IQA** | CLIP-based perceptual quality |

**Implementation:** `src/metrics/image_quality.py` - Laplacian-based quality estimation

---

### B. Alignment Metrics
**Purpose:** Measure text-image correspondence

#### Model-Based (Fast)
| Metric | What It Measures | Best For |
|--------|------------------|----------|
| **CLIPScore** | Global semantic alignment (embedding cosine similarity) | Overall intent match |
| **VQAScore** | Visual QA-based verification | Factual correctness |
| **AHEaD** | CLIP attention-based alignment | Fine-grained matching |
| **PickScore** | Human preference estimation | Subjective quality |

#### VLM-Based (GPT-4o)
| Metric | What It Measures | Best For |
|--------|------------------|----------|
| **TIFA** | QA pair verification | Factual accuracy |
| **DSG** | Davidsonian semantic primitives | Structured verification |
| **PSG** | Scene graph structure | Object relationships |
| **VPEval** | Visual programming evaluation | Compositional reasoning |

**Implementation:** `src/metrics/alignment.py` - CLIP/ViLT models + GPT-4o VLM

---

### C. Safety Metrics (T2ISafety Framework)
**Purpose:** Responsible AI evaluation

| Metric | What It Measures |
|--------|------------------|
| **Toxicity** | Harmful content (hate speech, violence, NSFW) |
| **Fairness** | Bias and stereotyping |
| **Privacy** | Identifiable information |

**Implementation:** `src/metrics/safety.py` - GPT-4o VLM evaluation

---

## üéØ Evaluation Workflow

### Single Image Evaluation
```
1. Calculate Soft-TIFA GM (North Star)
   - Extract atomic criteria from prompt
   - Evaluate each atom probabilistically
   - Calculate geometric mean
   ‚Üì
2. Run T2ISafety evaluation
   ‚Üì
3. Calculate Image Quality metrics (BRISQUE, NIQE, CLIP-IQA)
   ‚Üì
4. Calculate Model-based Alignment metrics (CLIPScore, VQAScore, AHEaD, PickScore)
   ‚Üì
5. Calculate VLM-based Alignment metrics (TIFA, DSG, PSG, VPEval)
   ‚Üì
6. Run Expert VLM evaluation (GPT-4o qualitative assessment)
   ‚Üì
6. Generate comprehensive report
```

### Batch Evaluation
```
For each image:
1. Extract atoms ‚Üí Evaluate atoms ‚Üí Calculate Soft-TIFA GM
2. Record results with all metrics

Summary:
- Average Soft-TIFA GM (primary metric)
- Per-image breakdown
```

---

## üìà Report Structure

The report is organized in this order:

1. **‚≠ê North Star Metric** - Soft-TIFA GM score prominently displayed
2. **üî¨ Soft-TIFA Atomic Fact Verification** - Detailed breakdown of each criterion
3. **üí° Expert VLM Evaluation** - GPT-4o subjective assessment
4. **üéØ Alignment Metrics** - Model-based (CLIPScore, VQAScore, AHEaD, PickScore) + VLM-based (TIFA, DSG, PSG, VPEval)
5. **üñºÔ∏è Image Quality Metrics** - BRISQUE, NIQE, CLIP-IQA
6. **üõ°Ô∏è Safety Metrics** - Toxicity, Fairness, Privacy
7. **üìä Overall Summary** - Category averages

*Performance metrics are displayed separately under the generated image.*

---

## üîß Implementation Details

### Project Structure
```
src/
‚îú‚îÄ‚îÄ app.py              # Main Gradio application
‚îú‚îÄ‚îÄ openai_service.py   # DALL-E 3 image generation
‚îî‚îÄ‚îÄ metrics/            # Shared metrics module
    ‚îú‚îÄ‚îÄ __init__.py     # Module exports
    ‚îú‚îÄ‚îÄ utils.py        # Utilities (pil_to_base64, model loaders)
    ‚îú‚îÄ‚îÄ soft_tifa.py    # North Star metric
    ‚îú‚îÄ‚îÄ image_quality.py # BRISQUE, NIQE, CLIP-IQA
    ‚îú‚îÄ‚îÄ alignment.py    # CLIPScore, VQAScore, AHEaD, PickScore, TIFA, DSG, PSG, VPEval
    ‚îî‚îÄ‚îÄ safety.py       # T2ISafety evaluation
```

### Current Implementation
- **Soft-TIFA GM**: VLM-based atom extraction and verification (GPT-4o)
- **CLIPScore**: Real CLIP embeddings cosine similarity
- **VQAScore**: Real ViLT model question answering
- **AHEaD**: CLIP attention-based alignment metric
- **TIFA/DSG/PSG/VPEval**: VLM-based alignment metrics (GPT-4o)
- **Image Quality**: Laplacian-based estimation (OpenCV)
- **Safety**: GPT-4o VLM evaluation

---

## üéì When to Use Each Metric

### Soft-TIFA GM (Always Report First)
- **Use for:** Primary quality indicator, benchmark comparisons
- **Strengths:** Fine-grained correctness, compositional logic

### Image Quality Metrics
- **Use for:** Technical assessment independent of prompt
- **Best for:** Debugging generation quality, comparing models

### Alignment Metrics
- **CLIPScore:** Quick global semantic assessment
- **VQAScore:** Factual verification via Q&A
- **AHEaD:** Fine-grained CLIP-based alignment
- **TIFA/DSG/PSG/VPEval:** VLM-based structured verification

### Safety Metrics
- **Use for:** Responsible AI evaluation
- **Best for:** Production deployment, bias detection

---

## üìä Metric Selection Guide

| Task | Primary Metric | Supporting Metrics |
|------|----------------|-------------------|
| **Benchmark T2I Models** | Soft-TIFA GM | CLIPScore, Image Quality |
| **Debug Generation Quality** | Image Quality | CLIP-IQA, VLM Quality Estimate |
| **Verify Prompt Adherence** | Soft-TIFA GM | VQAScore, Attribute Accuracy |
| **Evaluate Complex Prompts** | Soft-TIFA GM | VLM-as-a-Judge (Reasoning) |
| **Compare Model Outputs** | Soft-TIFA GM | All Alignment + Quality |
| **Style Transfer** | LPIPS | Image Quality |

---

## üöÄ Quick Start

### 1. Run the Application
```powershell
# Activate the conda environment
conda activate t2i_grader

# Or use the Python executable directly
C:\Users\hshuj\.conda\envs\t2i_grader\python.exe src/app.py
```

### 2. Single Image Evaluation
- Upload image + enter prompt ‚Üí Click "Grade Image Quality"
- Get comprehensive report with all metrics

### 3. Batch Benchmarking
Upload CSV with `prompt` column (optional `image_path`):
```csv
prompt
"A red apple on a wooden table"
"A steampunk workshop with brass gears"
```

---

## üìö References

**Soft-TIFA:**
- Hu et al. "TIFA: Accurate and Interpretable Text-to-Image Faithfulness Evaluation with Question Answering"

**Image Quality:**
- BRISQUE: Mittal et al. "No-Reference Image Quality Assessment in the Spatial Domain"
- NIQE: Mittal et al. "Making a Completely Blind Image Quality Analyzer"

**Alignment:**
- CLIPScore: Hessel et al. "CLIPScore: A Reference-free Evaluation Metric for Image Captioning"
- VQAScore: Lin et al. "Evaluating Text-to-Visual Generation with Image-to-Text Generation"

**Safety:**
- T2ISafety: Framework for responsible AI evaluation in text-to-image generation

---

## ‚úÖ Summary

**Key Design Decisions:**
1. ‚≠ê **North Star First:** Soft-TIFA GM reported prominently
2. üìä **Hierarchical Structure:** North Star ‚Üí VLM Evaluation ‚Üí Alignment ‚Üí Quality ‚Üí Safety
3. üîß **Modular Implementation:** Shared `metrics/` module for all metric calculations
4. üéØ **Use-Case Driven:** Different metrics for different evaluation needs

**Architecture:**
- Single entry point: `app.py`
- Shared metrics module: `metrics/`
- Performance metrics displayed under the image
- Comprehensive report on the right side
